# petbot using ROS1

#Project Details

PetBot is a simulated robotic pet that interacts with humans in a discrete 2D environment. The robot can exhibit three primary behaviors:

Normal Behavior: In this mode, the robot moves randomly around the environment, exploring its surroundings.

Sleep Mode: The robot returns to its home position, enters a sleep state for a specified duration, and then resumes normal behavior. This feature allows for a realistic rest period, enhancing its pet-like qualities.

Play Mode: When activated, the robot approaches the human's location and awaits a pointing gesture. Upon receiving the gesture, it moves to the specified location, returns to the human, and remains poised for the next interaction. After a set duration in play mode, the robot transitions back to its normal behavior.

# Behavioral Architecture
Assuming a Robot, simulating a pet, that interacts with a human and moves in a discrete 2D environment. 

The robot may have three behaviors : normal (in which it moves randomly), sleep (in which it gets the home position, sleeps for a time and returns in normal behavior) and play (in which it goes in person location, waits for a poiting gesture, goes in the pointed location, comes back to the person and waits for the next pointing gestures. After some time it returns to the normal behavior).

The human can interact by pointing gestures and speech. 

## ROS Architecture
The system is composed by 3 nodes:"Commander" node, "state_machine" node and "Display" node, and a Launch file. 
The rqt_graph is showed.
 
<p align="center"> 
<img src="https://github.com/SaiVinay023/Exp_Ass1/blob/main/Images/rqt_graph2.png">
</p>

### Commander node 
The node simulates user action. 
Firstly it chooses randomly what the robot must perform: sleep or play. The command (std_msgs/String) is published on a Topic ("/Command"). 
In case the play behavior is selected, the node generates randomly the location of the user and the Pointing Gesture. 
These poses (geometry_msgs/Point) are published on two topics ("/PersonPosition" and "/PointingGesture"). 

### State machine node
The node implements a state machine in which the three possible behaviors are defined. 

<p align="center"> 
<img src="https://github.com/SaiVinay023/Exp_Ass1/blob/main/Images/state%20machine.png">
</p>

The state machine starts with a Normal behavior state, and it can transit to sleep state or to play state, or it can keep a normal state.
The command to switch to another state is received by "Commander" node by the topic "/Command". 
When it is in a Play state or in a Sleep state, it can just go back in the normal state. 
The ROS message geometry_msgs/Point is used to rapresent the 2D robot's position (leaving z component always equal to 0).
The target position belongs exclusively to the map (11x11 grid). 
In the normal state, they are generated by "GenerateRandomPosition" function; in the play state, the person position and pointing gesture are received by "Commander" node through two topics; while in the sleep state the home position is defined (1,1). 
For each state, the target positions are sent to the "display" node, publishing them on "NewTargetPosition" topic.
When this happens, the system waits for a default time (4 seconds) until the robot reaches the desired pose. This means that further targets has not been accepted while the robot is moving. 

### Display node
The node is a simple simulator; it subscribes to a "newTargetPosition" topic and wait for a message. 
When the target is received, it prints on terminal what the robot is doing and the behavior state. 

## Installation
The first thing to do, after having cloned the repository in the Ros workspace, is to build the package, using the following commands in the shell:
    
    ```
    cd "yourWorkspace"_ws
    catkin_make

    ```
To run the system:
    
    ```
    roslaunch assignment1_ExRoLab ass1.launch
    
    ```
In another terminal run: 

    ```
    rosrun assignment1_ExRoLab Commander.py 
    
    ```

To visualize the smach viewer: 

    ```
    rosrun smach_viewer smach_viewer.py
    
    ```
##

   
## Working Hypothesis 
The assumptions are: 
1) When it starts, the normal state preempts other states. 
2) From the normal behavior, the robot can play or sleep or stay in normal state yet. 
3) The command to go to sleep or play is received by the Commander node. 
4) From the play state, as well as from the sleep state, the robot can only go to the normal state. 
5) The random position in the normal state is defined by "GenerateRandomPosition" function. 
6) The home position is fixed (1,1,0). 
7) The person location and the pointing gesture are received by the Commander node. 
8) During the execution of one state, after giving a target, the robot waits for a while (predefined time).

##**Interaction**
The human can engage with PetBot through simple pointing gestures and speech commands, making the experience intuitive and engaging. This design fosters a playful and interactive relationship between the robot and its human counterpart, simulating the companionship of a real pet.

Tools Used

Programming Language: Python

Libraries: Pygame for environment simulation, SpeechRecognition for voice interaction, and OpenCV for gesture recognition.

Framework: ROS (Robot Operating System) for integrating robotic functionalities.
Technical Implementation Methodology

Environment Setup: Create a discrete 2D grid environment using Pygame to visualize the robot's movements and interactions.

Behavioral Logic: Implement state management to handle the three distinct behaviors of the robot, including timers for sleep and transition logic between states.

User Interaction: Develop gesture and speech recognition modules to enable seamless interaction, allowing the robot to respond appropriately to human commands.

Simulation and Testing: Conduct thorough testing of the robot's behaviors and interaction capabilities to ensure a smooth and engaging user experience.

This project aims to provide a captivating simulation of a robotic pet, blending fun, interactivity, and technology.

## System's features 
The sys can show on the terminal what the robot is doing and the what the behavior's state is executing. 


## System's Limitations 
The user actions is randomly defined by the system, and there exists neither a real relationship with the user nor a real pointing gesture. 
The robot has not a velocity to get him desired target position, and there is not a real simulator to view if the target is achieved. 


## Possible technical Improvements 

A possible technical improvements may be: 
1) create a real interaction with the user; 
2) Implement a real velocity to change the odometry of the robot; 
4) Implement a simulator (like Gazebo, Stage, Turtlesim,...). 

## Author 
S4845876 - Saivinay Manda
saivinay023@gmail.com





